[{"authors":["ChaoruiDeng"],"categories":null,"content":"Chaorui is a first-year PhD student of the School of Computer Science, University of Adelaide. His research interests including image/video understanding and joint vision-language modeling.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"67b7210bf8ef526416a5f369a4436433","permalink":"/author/chaorui-deng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chaorui-deng/","section":"authors","summary":"Chaorui is a first-year PhD student of the School of Computer Science, University of Adelaide. His research interests including image/video understanding and joint vision-language modeling.","tags":null,"title":"Chaorui Deng","type":"authors"},{"authors":["ChongyangZhao"],"categories":null,"content":"Chongyang is a first-year MPhil student of the School of Computer Science, University of Adelaide, he is under the supervision of Dr. Qi Wu. Before that he obtained his bachelor degree from Beijing Jiaotong University, supervised by Prof. Runmin Cong. His research interests include computer vision and deep learning. Recently, he is focusing on image segmentation, object detection and vision \u0026amp; language problems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c0431afd4033dab2c3c1f8ebadd1c594","permalink":"/author/chongyang-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chongyang-zhao/","section":"authors","summary":"Chongyang is a first-year MPhil student of the School of Computer Science, University of Adelaide, he is under the supervision of Dr. Qi Wu. Before that he obtained his bachelor degree from Beijing Jiaotong University, supervised by Prof.","tags":null,"title":"Chongyang Zhao","type":"authors"},{"authors":["CristianRodriguezOpazo"],"categories":null,"content":"xxx is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a229959edebcfbdc9d7caa09c02c7ea6","permalink":"/author/cristian-rodriguez-opazo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cristian-rodriguez-opazo/","section":"authors","summary":"xxx is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Cristian Rodriguez Opazo","type":"authors"},{"authors":["MahdiKazemiMoghaddam"],"categories":null,"content":"Mahdi is a Ph.D student at the Australian Institute for Machine Learning in Adelaide, supervised by Prof. Javen Shi, Dr. Qi Wu and Dr. Eshan Abbasnejad. Previously, he finished his Honours Degree of Bachelor of Computer Science at the University of Adelaide, with First class Honours, where he was awarded the faculty’s Valedictorian (among eight schools) and the Dean’s Academic Excellence Award for his outstanding achievements. His research interests include visual navigation and reinforcement learning in general.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d6d24c4a7d2af7d31c34c5f528174bfd","permalink":"/author/mahdi-kazemi-moghaddam/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mahdi-kazemi-moghaddam/","section":"authors","summary":"Mahdi is a Ph.D student at the Australian Institute for Machine Learning in Adelaide, supervised by Prof. Javen Shi, Dr. Qi Wu and Dr. Eshan Abbasnejad. Previously, he finished his Honours Degree of Bachelor of Computer Science at the University of Adelaide, with First class Honours, where he was awarded the faculty’s Valedictorian (among eight schools) and the Dean’s Academic Excellence Award for his outstanding achievements.","tags":null,"title":"Mahdi Kazemi Moghaddam","type":"authors"},{"authors":["Qi Chen"],"categories":null,"content":"Qi Chen is a first-year Ph.D. student in the School of Computer Science at the University of Adelaide. He is under the supervision of Dr. Qi Wu and Dr. Yuankai Qi. His research interests include image/video generation and multi-modal visual synthesis. Currently, he is focusing on the problem of controllable image/video synthesis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"11a62f193340e675da3132f4610a165e","permalink":"/author/qi-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qi-chen/","section":"authors","summary":"Qi Chen is a first-year Ph.D. student in the School of Computer Science at the University of Adelaide. He is under the supervision of Dr. Qi Wu and Dr. Yuankai Qi.","tags":null,"title":"Qi Chen","type":"authors"},{"authors":["QiWu"],"categories":null,"content":"My research interests are mainly in computer vision and machine learning. My previous research projects include modeling visual objects regardless of depictive styles and image understanding using contextual cues. I am currently leading a small team at the Adelaide to research on the topic of Vision-and-Language.\nI have been in the computer vision filed for nearly 6 years and I have a strong track record in this field. Currently, I am working on the vision to language problem and I am especially an expert in the image captioning and visual question answering (VQA). In 2015, my image captioning model and VQA model achieved the leading performance in the Microsoft COCO Image Captioning Challenges and VQA Challenges. I have published several papers in the top journals such as IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), IEEE Signal Processing Magazine (SPM), Computer Vision and Image Understanding (CVIU). I also have published several papers on the top conference, such as International Joint Conference on Artificial Intelligence (IJCAI), AAAI, The Conference on Computer Vision and Pattern Recognition (CVPR) and the European Conference on Computer Vision (ECCV), and so on.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"96138a908bad730e7775053554425355","permalink":"/author/qi-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qi-wu/","section":"authors","summary":"My research interests are mainly in computer vision and machine learning. My previous research projects include modeling visual objects regardless of depictive styles and image understanding using contextual cues. I am currently leading a small team at the Adelaide to research on the topic of Vision-and-Language.","tags":null,"title":"Qi Wu","type":"authors"},{"authors":["ShizhenChen"],"categories":null,"content":"xxx is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8ce1f0e9f2deab91bd70b11de7bfa8d4","permalink":"/author/shizhe-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shizhe-chen/","section":"authors","summary":"xxx is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Shizhe Chen","type":"authors"},{"authors":["YanyuanQiao"],"categories":null,"content":"Yanyuan is a first-year PhD student of the School of Computer Science of the University of Adelaide, she is under the supervision of Dr. Qi Wu and Dr. Yuankai Qi. Yanyuan has a broad research interests in computer vision and natural language processing. Currently, she is focusing on the problem of General Vision and Language Methods in Real Applications.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"52d558c47cf8f0bb493d82093d89c527","permalink":"/author/yanyuan-qiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yanyuan-qiao/","section":"authors","summary":"Yanyuan is a first-year PhD student of the School of Computer Science of the University of Adelaide, she is under the supervision of Dr. Qi Wu and Dr. Yuankai Qi. Yanyuan has a broad research interests in computer vision and natural language processing.","tags":null,"title":"Yanyuan Qiao","type":"authors"},{"authors":["YicongHong"],"categories":null,"content":"Yicong is a second-year PhD student of the College of Engineering and Computer Science of the Australian National University (ANU), he is under the supervision of Prof. Stephen Gould and Dr. Qi Wu. Yicong has a broad research interests in computer vision, natural language processing and robotics. Currently, he is focusing on the problem of Vision-and-Language Navigation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7de4e038ab49d499a5a3029d5b33f5af","permalink":"/author/yicong-hong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yicong-hong/","section":"authors","summary":"Yicong is a second-year PhD student of the College of Engineering and Computer Science of the Australian National University (ANU), he is under the supervision of Prof. Stephen Gould and Dr.","tags":null,"title":"Yicong Hong","type":"authors"},{"authors":["YuankaiQi"],"categories":null,"content":"During my Ph.D period, I mainly fouced on the visual object tracking task. I have published many papers regarding visual tracking on top venues, such as TPAMI, TIP, CVPR, ECCV, AAAI, etc. I have also achieved serval awards on visual tracking related international challenges, such as the Winner of DAVIS 2017, the runner-up of VisDrone 2018.\nNow, I focus more on vision-and-language navigation tasks and have published several papers on CVPR, ECCV, NeurIPS. In 2019, we proposed the remote object grounding task in real-indoor envirionments, hoping push forward the research of intelligent robots. Currently, I serve as reviewers for TPAMI, TIP, CVIU, TCSVT, PR, CVPR, AAAI, ICCV, etc.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bc3460e7dda3f59c4ff4ab2b1ece54ed","permalink":"/author/yuankai-qi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuankai-qi/","section":"authors","summary":"During my Ph.D period, I mainly fouced on the visual object tracking task. I have published many papers regarding visual tracking on top venues, such as TPAMI, TIP, CVPR, ECCV, AAAI, etc.","tags":null,"title":"Yuankai Qi","type":"authors"},{"authors":["Yutong Xie"],"categories":null,"content":"Yutong is currently a Postdoctoral Research Fellow at Australian Institute for Machine Learning (AIML), part of The University of Adelaide (UoA), working with Dr. Qi Wu. She received her Ph.D. in the School of Computer Science and Engineering at Northwestern Polytechnical University (NPU), China, under the supervision of Prof. Yong Xia. She has also spent two years at UoA during her Ph.D. studies, under the supervision of Prof. Chunhua Shen and Dr. Johan Verjans.\nYutong\u0026rsquo;s focus is mainly on artificial intelligence in medical data analysis applications, aiming to develop smart solutions to assist medical professionals in the lesion and anatomical structure segmentation, and disease diagnosis and therapy. Recently, she has been investigating self-supervised/multi-modal learning to improve medical data analysis with limited annotations. Yutong has published over 10 peer-reviewed articles in top-tier journals/conferences such as IEEE-TMI, Medical Image Analysis, CVPR, IJCAI, MICCAI, and so on.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f2f586201df651075fe2b80ba41e7b0d","permalink":"/author/yutong-xie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yutong-xie/","section":"authors","summary":"Yutong is currently a Postdoctoral Research Fellow at Australian Institute for Machine Learning (AIML), part of The University of Adelaide (UoA), working with Dr. Qi Wu. She received her Ph.D. in the School of Computer Science and Engineering at Northwestern Polytechnical University (NPU), China, under the supervision of Prof.","tags":null,"title":"Yutong Xie","type":"authors"},{"authors":["ZhenfangChen"],"categories":null,"content":"xxx is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"47d894155e94942c487e36b961cb9c85","permalink":"/author/zhenfang-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhenfang-chen/","section":"authors","summary":"xxx is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Zhenfang Chen","type":"authors"},{"authors":["ZhengYu"],"categories":null,"content":"Zheng Yu is a first-year PhD student of School of Computer Science at the University of Adelaide, under the supervision of Dr. Qi Wu. Zheng Yu has a broad research interest in computer vision and natural language processing. Currently, he is focusing on the problem of Medical Visual Question Answering.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a766c333f4715354a7d573d8c8ed11c0","permalink":"/author/zheng-yu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zheng-yu/","section":"authors","summary":"Zheng Yu is a first-year PhD student of School of Computer Science at the University of Adelaide, under the supervision of Dr. Qi Wu. Zheng Yu has a broad research interest in computer vision and natural language processing.","tags":null,"title":"Zheng Yu","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Yicong Hong","Qi Wu","Yuankai Qi","Cristian Rodriguez-Opazo","Stephen Gould"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"6ac2f313905fc25f4eb75298728de45d","permalink":"/publication/63/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/63/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"A Recurrent Vision-and-Language BERT for Navigation","type":"publication"},{"authors":["Zeren Sun","Yazhou Yao","Fumin Shen","Qi Wu","Zhenmin Tang","Jian Zhang"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"9063a21d360dc2ad1340b77f37316ef3","permalink":"/publication/66/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/66/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Jo-SRC: A Contrastive Approach for Combating Noisy Labels","type":"publication"},{"authors":["Tao Chen","Guo-Sen Xie","Yazhou Yao","Fumin Shen","Qi Wu","Zhenmin Tang","Jian Zhang"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"afc264f30e9943665cd21d7b7468b9bf","permalink":"/publication/67/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/67/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Non-Salient Region Object Mining for Weakly Supervised Semantic Segmentation","type":"publication"},{"authors":["Chen Gao","Jinyu Chen","Si Liu","Luting Wang","Qiong Zhang","Qi Wu"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"ba970b4e3fed745c42261fd66e80d067","permalink":"/publication/65/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/65/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Room-and-Object Aware Knowledge Reasoning for Remote Embodied Referring Expression","type":"publication"},{"authors":["Chaorui Deng","Shizhe Chen","Da Chen","Qi Wu"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"c01dde6355c5b051b4687c29f05a8ba7","permalink":"/publication/62/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/62/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Sketch, Ground, and Refine: Top-Down Dense Video Captioning","type":"publication"},{"authors":["Guanghui Xu","Mingkui Tan","Shuaicheng Niu","Yucheng Luo","Qing Du","Qi Wu"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"33d7f8b3325a4f71d29159d671b9a72c","permalink":"/publication/64/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/64/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Towards Accurate Text-based Image Captioning with Content Diversity Exploration","type":"publication"},{"authors":["Li Liu","Mengge He","Guanghui Xu","Mingkui Tan","Qi Wu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"9f1469da2f3587f052bb89594723e810","permalink":"/publication/16/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/16/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"How to Train Your Agent to Read and Write?","type":"publication"},{"authors":["Mahdi Kazemi Moghaddam","Qi Wu","Ehsan Abbasnejad","Javen Qinfeng Shi"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"9476e67b9b6aef2a57801e04b475844f","permalink":"/publication/18/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/18/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Optimistic Agent: Accurate Graph-Based Value Estimation for More Successful Visual Navigation","type":"publication"},{"authors":["Qi Zhu","Chenyu Gao","Peng Wang","Qi Wu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"f62988b1710e91d434ce7525aebbb838","permalink":"/publication/17/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/17/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps.","type":"publication"},{"authors":["Yicong Hong","Cristian Rodriguez-Opazo","Yuankai Qi","Qi Wu","Stephen Gould"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"f01dda0acb3a0a5591286b29aace96d4","permalink":"/publication/20/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/20/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Language and Visual Entity Relationship Graph for Agent Navigation","type":"publication"},{"authors":["Jing Yu","Xiaoze Jiang","Zengchang Qin","Weifeng Zhang","Yue Hu","Qi Wu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"8eb2070c9756a7c670072b1a7052cdc3","permalink":"/publication/2/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/2/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue","type":"publication"},{"authors":["Yuankai Qi","Zizheng Pan","Shengping Zhang","Anton van den Hengel","Qi Wu"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"c300105b050e0b67e0b4aa2c92619ab4","permalink":"/publication/27/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/27/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Object-and-Action Aware Model for Visual Language Navigation","type":"publication"},{"authors":["Jing Yu","Weifeng Zhang","Yuhang Lu","Zengchang Qin","Yue Hu","Jianlong Tan","Qi Wu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"e86d11e4e78f0023a051dcbc76a02c05","permalink":"/publication/5/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/5/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Reasoning on the Relation: Enhancing Visual Representation for Visual Question Answering and Cross-modal Retrieval","type":"publication"},{"authors":["Yuankai Qi","Qi Wu","Peter Anderson","Xin Wang","William Yang Wang","Chunhua Shen","Anton van den Hengel"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"ad50ddc18431cc81098f81d727e7ac37","permalink":"/publication/37/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/37/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments","type":"publication"},{"authors":["Zhenfang Chen","Peng Wang","Lin Ma","Kwan-Yee K. Wong","Qi Wu"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"d2ad92a8fe1acddf9099c1da463890d6","permalink":"/publication/32/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/32/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension","type":"publication"},{"authors":["Xiaoze Jiang","Jing Yu","Yajing Sun","Zengchang Qin","Zihao Zhu","Yue Hu","Qi Wu"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"d63c957a332d8009cb59a667696e9f28","permalink":"/publication/30/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/30/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"DAM: Deliberation- Abandon and Memory Networks for Generating Detailed and Non-repetitive Responses in Visual Dialogue","type":"publication"},{"authors":["Xiaoze Jiang","Jing Yu","Zengchang Qin","Yingying Zhuang","Xingxing Zhang","Yue Hu","Qi Wu"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"8b09e652633d6eb66b2dc6705fe18720","permalink":"/publication/39/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/39/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue","type":"publication"},{"authors":["Peng Wang","Dongyang Liu","Hui Li","Qi Wu"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"62d027b47c5253dfe150f2d98b328bef","permalink":"/publication/24/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/24/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Give Me Something to Eat: Referring Expression Comprehension with Commonsense Knowledge","type":"publication"},{"authors":["Weixia Zhang","Chao Ma","Qi Wu","Xiaokang Yang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"b3097097d85848fb32f500f754dba046","permalink":"/publication/6/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/6/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Language-guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning","type":"publication"},{"authors":["Chaorui Deng","Ning Ding","Mingkui Tan","Qi Wu"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"2f19c22e7ce3ab1b427109e4128afa9d","permalink":"/publication/28/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/28/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Length Controllable Image Captioning","type":"publication"},{"authors":["Yihan Zheng","Zhiquan Wen","Mingkui Tan","Runhao Zeng","Qi Chen","Yaowei Wang","Qi Wu"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"f208b79b95044ce5efeaa128f74ec864","permalink":"/publication/40/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/40/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Modular Graph Attention Network for Complex Visual Relational Reasoning","type":"publication"},{"authors":["Zihao Zhu","Jing Yu","Yujing Wang","Yajing Sun","Yue Hu","Qi Wu"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"bbc2eb749c474e7fa8484bbefac3fd51","permalink":"/publication/31/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/31/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering","type":"publication"},{"authors":["Shizhe Chen","Qin Jin","Peng Wang","Qi Wu"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"ce5171eeac3a69208db0028dec2d3acc","permalink":"/publication/35/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/35/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs","type":"publication"},{"authors":["Ruixue Tang","Chao Ma","Wei Emma Zhang","Qi Wu","Xiaokang Yang"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"4c25b2c89310aa30cf192cbda8192b6f","permalink":"/publication/29/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/29/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering","type":"publication"},{"authors":["Hu Wang","Qi Wu","Chunhua Shen"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"43cd9ddbc9253a5a28d9b20c8d4e0821","permalink":"/publication/26/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/26/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Soft Expert Reward Learning for Vision-and-Language Navigation","type":"publication"},{"authors":["Yicong Hong","Cristian Rodriguez-Opazo","Qi Wu","Stephen Gould"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"cfa3652f92dc29951f8126c851a08c80","permalink":"/publication/21/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/21/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Sub-Instruction Aware Vision-and-Language Navigation","type":"publication"},{"authors":["Fan Lyu","Qi Wu","Fuyuan Hu","Qingyao Wu","Mingkui Tan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"a681461fae55d5dacc4c270d16511ffe","permalink":"/publication/8/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/8/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Attend and Imagine: Multi-label Image Classification with Visual Attention and Recurrent Neural Networks","type":"publication"},{"authors":["Fen Liu","Guanghui Xu","Qi Wu","Qing Du","Wei Jia","Mingkui Tan"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"e59f7b1ecf781be5b400bdbbda60583f","permalink":"/publication/22/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/22/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Cascade Reasoning Network for Text-based Visual Question Answering","type":"publication"},{"authors":["Peng Wang","Qi Wu","Chunhua Shen","Anthony Dick","Anton van den Hengel"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"de4ca9ffc06fb2e510d9e62536902337","permalink":"/publication/13/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/13/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"FVQA: Fact-based visual question answering","type":"publication"},{"authors":["Zhibin Liao*","Lingqiao Liu","Qi Wu","Damien Teney","Chunhua Shen","Johan Verjans","Anton van Hengel"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"1c0b8ff4b0521ae4cfb399c264c2d123","permalink":"/publication/41/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/41/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Medical Data Inquiry Using a Question Answering Model","type":"publication"},{"authors":["Chengchen Jing","Yuwei Wu","Mingtao Pei","Yao Hu","Yunde Jia","Qi Wu"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"bc40d9029b38a430c4d3cfd43c42511b","permalink":"/publication/23/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/23/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Visual-Semantic Graph Matching for Visual Grounding","type":"publication"},{"authors":["Chaorui Deng","Qi Wu","Qingyao Wu","Fuyuan Hu","Fan Lyu","Mingkui Tan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"e35ee5d6a2d0b25ff2db4054cbb402aa","permalink":"/publication/1/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/1/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Visual Grounding via Accumulated Attention","type":"publication"},{"authors":["Chuanyi Zhang","Yazhou Yao","Xiangbo Shu","Zechao Li","Zhenmin Tang","Qi Wu"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"67b33ac485c4102aebd12bb1067e4cc5","permalink":"/publication/25/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/25/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Data-driven Meta-set Based Fine-Grained Visual Classification","type":"publication"},{"authors":["Shizhe Chen","Yida Zhao","Qin Jin","Qi Wu"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"e96c44ae59c2c3f6fa71af0c52299426","permalink":"/publication/34/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/34/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning","type":"publication"},{"authors":["Ehsan Abbasnejad","Qi Wu","Iman Abbasnejad","Javen Shi","Anton van den Hengell"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"2f838d5a60b519f9f8de537a4467b69d","permalink":"/publication/36/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/36/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Gold Seeker: Information Gain from Policy Distributions for Goal-oriented Vision-and-Langauge Reasoning","type":"publication"},{"authors":["Yan Huang","Qi Wu","Wei Wang","Liang Wang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"7af5f6c6ae65e6649c08f65c71d2e54f","permalink":"/publication/10/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/10/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Image and Sentence Matching via Semantic Concepts and Order Learning","type":"publication"},{"authors":["Qi Chen*","Qi Wu* (equal contribution)","Rui Tang","Yuhan Wang","Shuai Wang","Mingkui Tan"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"6c28c6866fb41e931313d4c63156b030","permalink":"/publication/33/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/33/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Intelligent Home 3D: Automatic 3D-House Design from Linguistic Descriptions Only","type":"publication"},{"authors":["Qi Chen","Qi Wu","Jian Chen","Qingyao Wu","Anton van den Hengel","Mingkui Tan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"0dd5b0ac39124b270654ab011a52f7d9","permalink":"/publication/3/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/3/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Scripted Video Generation with a Bottom-up Generative Adversarial Network","type":"publication"},{"authors":["Yanyuan Qiao","Chaorui Deng","Qi Wu"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8b4dbaef88dd2e19cc11689be16ba2d5","permalink":"/publication/4/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/4/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Referring Expression Comprehension: A Survey of Methods and Datasets","type":"publication"},{"authors":["Chenchen Jing","Yuwei Wu","Xiaoxun Zhang","Yunde Jia","Qi Wu"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"7accb0cfb90222419bc5ea96196e66b4","permalink":"/publication/38/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/38/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Overcoming Language Priors in VQA via Decomposed Linguistic Representations","type":"publication"},{"authors":["Jianpeng Zhang","Yutong Xie","Qi Wu","Yong Xia"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"7b631e4361a243ff282aec387d00bc8f","permalink":"/publication/9/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/9/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Medical image classification using synergic deep learning","type":"publication"},{"authors":["Junjie Zhang","Qi Wu","Jian Zhang","Chunhua Shen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"600c6e9a5e5581965fd099b24be85ad7","permalink":"/publication/7/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/7/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Heritage Image Annotation via Collective Knowledge","type":"publication"},{"authors":["Junjie Zhang*","Qi Wu*","Chunhua Shen","Jian Zhang"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"0ff81c23c6cea7044cd7e61fdc798522","permalink":"/publication/11/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/11/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Multi-Label Image Classification with Regional Latent Semantic Dependencies","type":"publication"},{"authors":["Xuguang Duan","Qi Wu","Chuang Gan","Yiwei Zhang","Wenbing Huang","Anton van den Hengel","Wenwu Zhu"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"a7fc272470d81d43e3d75436d8eda681","permalink":"/publication/42/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/42/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Watch, Reason and Code: Learning to Represent Videos Using Program","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Charts Academic supports the popular Plotly chart format.\nSave your Plotly JSON in your page folder, for example chart.json, and then add the {{\u0026lt; chart data=\u0026quot;chart\u0026quot; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\n  (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./line-chart.json\", function(chart) { Plotly.plot('chart-921748365', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  You might also find the Plotly JSON Editor useful.\nMath Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^) = \\begin{cases} p_0^ \u0026amp; \\text{if }k=1, \\\\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Callouts Academic supports a shortcode for callouts, also referred to as asides, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Spoilers Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.\n{{\u0026lt; spoiler text=\u0026quot;Click to view the spoiler\u0026quot; \u0026gt;}} You found me! {{\u0026lt; /spoiler \u0026gt;}}  renders as\n Click to view the spoiler  You found me!    Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["unjie Zhang","Qi Wu","Jian Zhang","Chunhua Shen"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"346b478829d52eb8524468f5614aa587","permalink":"/publication/43/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/43/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Mind Your Neighbours: Image Annotation with Metadata Neighbourhood Graph Co-Attention Networks","type":"publication"},{"authors":["Qi Wu","Chunhua Shen","Anton van den Hengel","Peng Wang","Anthony Dick"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"ca95160359fdb57bcb60894a7273ae05","permalink":"/publication/12/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/publication/12/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge","type":"publication"},{"authors":["Qi Wu","Damien Teney","Peng Wang","Chunhua Shen","Anthony Dick","Anton van den Hengel"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"1a4aa3e4c45f2f0bdfc2d32bd6626401","permalink":"/publication/15/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/15/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Visual Question Answering: A Survey of Models and Datasets","type":"publication"},{"authors":["Damien Teney","Qi Wu","Anton van den Hengel"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"f3a861b95ca2be9dad2207cf865ce7d6","permalink":"/publication/14/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/14/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Journal"],"title":"Visual Question Answering: A Tutorial","type":"publication"},{"authors":["admin"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"89494ccfd31fb1c0910d7fd9dd53e353","permalink":"/job/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/job/","section":"","summary":"Job","tags":null,"title":"Job","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Hello!","tags":null,"title":"Landing Page","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"People","tags":null,"title":"People","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"Projects","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"23d1e75f872528fc12f5f2b142375ff7","permalink":"/publications/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publications/","section":"","summary":"Publications!","tags":null,"title":"Publications","type":"widget_page"},{"authors":["Jianpeng Zhang","Yutong Xie","Qi Wu","Yong Xia"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"e49b913477b5aba52971c37bdeeeed89","permalink":"/publication/51/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publication/51/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Skin Lesion Classification in Dermoscopy Images Using Synergic Deep Learning","type":"publication"},{"authors":["Qi Wu","Peng Wang","Chunhua Shen","Ian Reid","Anton van den Hengel"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"617fa48b47978a61b1ca3b300cbe9cdc","permalink":"/publication/45/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/45/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning","type":"publication"},{"authors":["Junjie Zhang","Qi Wu","Chunhua Shen","Jian Zhang","Jianfeng Lu"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"2403274d86b47ccd8be3d6090d9a19a1","permalink":"/publication/44/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/44/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Asking the Difficult Questions: Goal-Oriented Visual Question Generation via Intermediate Rewards","type":"publication"},{"authors":["Yan Huang","Qi Wu","Liang Wang"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"837c6541020ce4241c58d7ad32ff85de","permalink":"/publication/47/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/47/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Learning Semantic Concepts and Order for Image and Sentence Matching","type":"publication"},{"authors":["Bohan Zhuang","Qi Wu","Chunhua Shen","Ian Reid","Anton van den Hengel"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"31401108c401bece504a13b753fd5aa7","permalink":"/publication/46/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/46/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries","type":"publication"},{"authors":["Peter Anderson","Qi Wu","Damien Teney","Jake Bruce","Mark Johnson","Niko Sunderhauf","Ian Reid","Stephen Gould","Anton van den Hengel"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"fe4c5cde3a717f635f0f5c30846e6e62","permalink":"/publication/48/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/48/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments","type":"publication"},{"authors":["Chaorui Deng","Qi Wu","Fuyuan Hu","Fan Lv","Mingkui Tan"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"48bc6c228b56a4495a78f866b28e697f","permalink":"/publication/49/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/49/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Visual Grounding via Accumulated Attention","type":"publication"},{"authors":["Chao Ma","Chunhua Shen","Anthony Dick","Qi Wu","Peng Wang","Anton van den Hengel","Ian Reid"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"b3151153ae307514bdddca062943b091","permalink":"/publication/50/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/50/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Visual Question Answering with Memory-Augmented Networks","type":"publication"},{"authors":["Bohan Zhuang","Qi Wu","Ian Reid","Chunhua Shen","Anton van den Hengel"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"7b28b60dfdf2e4e8472ebd2d19dd84bc","permalink":"/publication/52/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/52/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"HCVRD: a benchmark for large-scale Human-Centered Visual Relationship Detection","type":"publication"},{"authors":["Junjie Zhang","Qi Wu","Jian Zhang","Chunhua Shen","Jianfeng Lu"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"570ed13bb76b89695bfe2eb6fcb4e556","permalink":"/publication/53/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/53/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Kill Two Birds With One Stone: Weakly-Supervised Neural Network for Image Annotation and Tag Refinement","type":"publication"},{"authors":["Peng Wang","Qi Wu","Chunhua Shen","Anton van den Hengel","Anthony Dick"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"45b61f1b8b3b82831bcba5c2d36ea661","permalink":"/publication/54/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/54/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Explicit Knowledge-based Reasoning for Visual Question Answering","type":"publication"},{"authors":["Peng Wang","Qi Wu","Chunhua Shen","Anton van den Hengel"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"ddc14cab2a73356b84d22e3246fe7010","permalink":"/publication/55/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/55/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions","type":"publication"},{"authors":["Qi Wu","Peng Wang","Chunhua Shen","Anton van den Hengel","Anthony Dick"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"43d9cd0f84bc93a4224c7693aa6ccd3b","permalink":"/publication/56/","publishdate":"2016-06-01T00:00:00Z","relpermalink":"/publication/56/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources","type":"publication"},{"authors":["Qi Wu","Chunhua Shen","Anton van den Hengel","Lingqiao Liu","Anthony Dick"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"6403c6d64d083df3442fee05734b38a8","permalink":"/publication/57/","publishdate":"2016-06-01T00:00:00Z","relpermalink":"/publication/57/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"What Value Do Explicit High Level Concepts Have in Vision to Language Problems? ","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"47a3f377d2455d0bd65d4ba1dc3e2ab2","permalink":"/project/r2r/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/r2r/","section":"project","summary":"Room to Room Navigation","tags":["VLN"],"title":"R2R","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"6c583ddc975f96cc394c3d9705a75eba","permalink":"/project/reverie/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/reverie/","section":"project","summary":"Remote Object Grounding","tags":["VLN"],"title":"REVERIE","type":"project"},{"authors":["admin","吳恩達"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic","开源"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Hongping Cai","Qi Wu","Peter Hall"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"28f7efc3d2c95f5c2b9449ab5ab834bd","permalink":"/publication/58/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/58/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Beyond Photo-Domain Object Recognition: Benchmarks for the Cross-Depiction Problem","type":"publication"},{"authors":["Hongping Cai","Qi Wu","Peter Hall"],"categories":null,"content":"","date":1409529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409529600,"objectID":"1c91039b448bb94559de5d6f103f0ba2","permalink":"/publication/59/","publishdate":"2014-09-01T00:00:00Z","relpermalink":"/publication/59/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Learning Graphs to Model Visual Objects across Different Depictive Styles","type":"publication"},{"authors":["Qi Wu","Peter Hall"],"categories":null,"content":"","date":1377993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377993600,"objectID":"5763eb3a43d5edf73bdd701646835b93","permalink":"/publication/60/","publishdate":"2013-09-01T00:00:00Z","relpermalink":"/publication/60/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Modelling Visual Objects Invariant to Depictive Style","type":"publication"},{"authors":["Qi Wu","Peter Hall"],"categories":null,"content":"","date":1346457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1346457600,"objectID":"70ac2554a051f47a18e2b7761f188c51","permalink":"/publication/61/","publishdate":"2012-09-01T00:00:00Z","relpermalink":"/publication/61/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Conference"],"title":"Learning Graphs to Model Visual Objects across Different Depictive Styles","type":"publication"}]